{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps with ``fluke`` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through the first steps with the ``fluke`` API. We will show how to quickly run an experiment using the API.\n",
    "\n",
    "Try this tutorial: [![Open in Colab](https://img.shields.io/badge/Open_in_Colab-blue?style=flat-square&logo=google-colab&logoColor=yellow&labelColor=gray)\n",
    "](https://colab.research.google.com/github/makgyver/fluke/blob/main/tutorials/fluke_quick_api.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation via pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't installed the package yet, you can do so by running the following command:\n",
    "\n",
    "```bash\n",
    "pip install fluke-fl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and splitting the dataset\n",
    "First of all, we need to load the dataset. Let say we want to load the `MNIST` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.data.datasets import Datasets\n",
    "dataset = Datasets.get(\"mnist\", path=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset` is a `DataContainer` which is a simple data structure containing the dataset as it is loaded from the files. The downloaded files are stored in the directory `path`. If the dataset is already downloaded, `path` can be set to the directory containing the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset, we need to prepare it for the distribution. For simplicity, let say that we use the test set provided by the dataset as the server-side test set (which will test the performance of the global model), and the training set as the client-side training set (which will be distributed to the clients). For now, we will use the default data distribution strategy, which is IID and client-side we do not have any test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.data import DataSplitter\n",
    "splitter = DataSplitter(dataset=dataset,\n",
    "                        distribution=\"iid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataSplitter` is the class responsible for splitting the dataset into the server-side and client-side datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the evaluator\n",
    "\n",
    "The evaluator is the class responsible for evaluating the performance of both the global and local models. \n",
    "It must be defined in the global setting of `fluke` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.evaluation import ClassificationEval\n",
    "from fluke import GlobalSettings\n",
    "\n",
    "evaluator = ClassificationEval(eval_every=1, n_classes=dataset.num_classes)\n",
    "GlobalSettings().set_evaluator(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using an evaluator for the classification task (to now the only one suppoerted).\n",
    "`eval_every` is the number of communication rounds after which the models are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and configure the federated learning algorithm\n",
    "\n",
    "Now, we are ready to instantiate our algorithm. We will go with the standard FedAvg but many others are available on ``fluke``.\n",
    "\n",
    "Instantiating a federated learning algorithm requires to set a bunch of hyper-parameters. ``fluke`` divides these parameters into two groups:\n",
    "\n",
    "1. *client-side*: the hyper-parameters of the clients which include the type of optimizer (and scheduler), learning rate, the number of local epochs, etc..\n",
    "2. *server-side*: hyper-parameters of the server, which are typically less than the clients' hyper-parameters, e.g., whether the aggregation is weighted or not.\n",
    "\n",
    "In the following code, we will set the hyper-parameters of the clients using a `DDict` that is a convenient data structure defined in ``fluke``. A simple dictionary can be used as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke import DDict\n",
    "client_hp = DDict(\n",
    "    batch_size=10,\n",
    "    local_epochs=5,\n",
    "    loss=\"CrossEntropyLoss\",\n",
    "    optimizer=DDict(\n",
    "      lr=0.01,\n",
    "      momentum=0.9,\n",
    "      weight_decay=0.0001),\n",
    "    scheduler=DDict(\n",
    "      gamma=1,\n",
    "      step_size=1)\n",
    ")\n",
    "\n",
    "# we put together the hyperparameters for the algorithm\n",
    "hyperparams = DDict(client=client_hp,\n",
    "                    server=DDict(weighted=True),\n",
    "                    model=\"MNIST_2NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may see, we need also to specify the model (i.e., the neural network) that will be used in the federated learning process. In this example, we will use the `MNIST_2NN` model which is a simple multi-layer perceptron with two hidden layers. The model is defined in the `nets` module of the `fluke` package.\n",
    "\n",
    "Finally, we are all set to create the federated learning algorithm. The `FedAvg` class is the implementation of the Federated Averaging algorithm. The `FedAvg` class requires the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.algorithms.fedavg import FedAVG\n",
    "algorithm = FedAVG(100, splitter, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the algorithm, we need to make sure to log the results. `fluke` is designed to allow different types of logging. For this reason, it implements the design pattern `Observer`. To attach a logger to the algorithm, we need to create an instance of the logger and attach it to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.utils.log import Log\n",
    "logger = Log()\n",
    "algorithm.set_callbacks(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Log` is a simple logger that logs the results in the console, while keeping the history of the results in a dictionaries.\n",
    "\n",
    "## Ready to go!\n",
    "\n",
    "Finally, we can run the algorithm. The `run` method of the algorithm requires to specify the number of rounds and the fraction of clients that will participate in each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "algorithm.run(2, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
