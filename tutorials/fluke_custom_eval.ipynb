{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom evaluation with `fluke`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through the steps required to implement a new evaluation that can be tested with ``fluke``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install `fluke` (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fluke-fl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted accuracy\n",
    "\n",
    "In this tutorial, we will show how to implement a metric that is quite common in Personalized Federated Learning in `fluke`! In particular, a common technique for evaluating the local model is through using a balanced test set, weighting the accuracy based on the number of samples of each class. Intuitively, the weighted accuracy takes into account the number of samples for each class, allowing to lower the penalty if an error occurs in classifying a less frequent class. This metric is taken from *Tackling Data Heterogeneity in Federated Learning with Class Prototypes*, Dai et al. and it is defined as follows:\n",
    "\n",
    "$$ acc_i = \\frac{\\sum_{x_j,y_j\\in D_{test}}\\alpha_i(y_j)\\mathbb{1}(y_j = \\hat{y}_j)}{\\sum_{x_j,y_j\\in D_{test}}\\alpha_i(y_j)} $$\n",
    "\n",
    "\n",
    "\n",
    "where $\\alpha_i(\\cdot)$ is a positive valued function. It is defined as the probability that the sample y is from class c in the $i^{th}$ client. Notice that, for $\\alpha_i(\\cdot) = 1$ we obtain the traditional accuracy. In this tutorial, we will interpret $\\alpha_i(\\cdot)$ as the proportion of the local samples of the class $y$ over all the sample of that client. Specifically, we calculate the aforementioned coefficient for client $i$ and class $y_j$ as $\\alpha_i(y_j) = \\frac{Y^i_j}{Y^i}$, where $Y^i_j$ is the number of samples of class $y_j$ for client $i$ (training set), and $Y^i$ is the total number of examples of client $i$.\n",
    "\n",
    "In our case $D_{test}$ will be the dataset on the server, that is (usually) the original test set of the dataset. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the metric\n",
    "\n",
    "In the following, we start from the `classification` metric present in `eval.py` and modify it, taking into account the weight for each class. As a sanity check, in the global evaluation `accuracy` and `weighted accuracy` will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import Metric\n",
    "\n",
    "class WeightedAccuracy(Metric):\n",
    "\n",
    "    def __init__(self, num_classes: int, weights: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.weights = weights\n",
    "        self.true_weights = []\n",
    "        self.pred_weights = []\n",
    "        self.mask = []\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor) -> None:\n",
    "        if preds.ndim == 2:\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "        self.true_weights.append(self.weights[target])\n",
    "        self.pred_weights.append(self.weights[preds])\n",
    "        self.mask.append(torch.eq(preds, target))\n",
    "\n",
    "    def compute(self) -> float:\n",
    "        true_weights = torch.cat(self.true_weights, dim=0)\n",
    "        pred_weights = torch.cat(self.pred_weights, dim=0)\n",
    "        mask = torch.cat(self.mask, dim=0)\n",
    "        pred_weights = pred_weights * mask\n",
    "        return pred_weights.sum() / true_weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the server-side logic\n",
    "\n",
    "Notice that `server.evaluation` is called in `server.fit` with only two arguments (the evaluator and the eligible clients). As a consequence, if we want to modify the `server.evaluate` to take into account the class weights, we should modify the `server.fit` as well. However, this is too verbose. The most straightforward solution is to not modify `server.fit` and the input arguments of `server.evaluate` function, but modify the evaluator `evaluator.evaluate` input arguments, adding the class weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from fluke.data import FastDataLoader  # NOQA\n",
    "from fluke.evaluation import Evaluator  # NOQA\n",
    "from fluke.client import Client  # NOQA\n",
    "from fluke.server import Server  # NOQA\n",
    "\n",
    "class MyServer(Server):\n",
    "\n",
    "    def evaluate(self, \n",
    "                 evaluator: Evaluator,\n",
    "                 test_set: FastDataLoader) -> dict[str, float]:\n",
    "        if test_set is not None:\n",
    "            return evaluator.evaluate(self.rounds + 1, \n",
    "                                      self.model, \n",
    "                                      test_set, \n",
    "                                      device=self.device, \n",
    "                                      additional_metrics={\n",
    "                                        \"weighted_accuracy\": WeightedAccuracy(\n",
    "                                        evaluator.n_classes, torch.ones(evaluator.n_classes)\n",
    "                                      )})\n",
    "        return {}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the client-side logic\n",
    "\n",
    "Following the same logic as the server, we modify the `evaluator.evaluate` instead of the whole `client.local_update` and the inputs of `client.evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from typing import Any\n",
    "\n",
    "from fluke.utils import OptimizerConfigurator  # NOQA\n",
    "\n",
    "class MyClient(Client):\n",
    "\n",
    "    def __init__(self,\n",
    "                 index: int,\n",
    "                 train_set: FastDataLoader,\n",
    "                 test_set: FastDataLoader,\n",
    "                 optimizer_cfg: OptimizerConfigurator,\n",
    "                 loss_fn: Module,\n",
    "                 local_epochs: int = 3,\n",
    "                 **kwargs: dict[str, Any]):\n",
    "        super().__init__(index,\n",
    "                 train_set,\n",
    "                 test_set,\n",
    "                 optimizer_cfg,\n",
    "                 loss_fn,\n",
    "                 local_epochs,\n",
    "                 **kwargs)\n",
    "        self.class_weights = torch.bincount(self.train_set.tensors[1]).float()\n",
    "        self.class_weights /= self.train_set.size\n",
    "\n",
    "        \n",
    "    def evaluate(self, \n",
    "                 evaluator: Evaluator,\n",
    "                 test_set: FastDataLoader) -> dict[str, float]: \n",
    "        if self.model is not None:\n",
    "            return evaluator.evaluate(self._last_round, \n",
    "                                      self.model, \n",
    "                                      test_set, \n",
    "                                      device=self.device, \n",
    "                                      additional_metrics={\n",
    "                                        \"weighted_accuracy\": WeightedAccuracy(\n",
    "                                        evaluator.n_classes, self.class_weights\n",
    "                                      )})\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the new metric\n",
    "\n",
    "Now, we are ready to test our metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.algorithms import CentralizedFL\n",
    "\n",
    "class MyFLAlgorithm(CentralizedFL):\n",
    "\n",
    "    def get_client_class(self) -> type[Client]:\n",
    "        return MyClient\n",
    "\n",
    "    def get_server_class(self) -> type[Server]:\n",
    "        return MyServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to test the new federated algorithm\n",
    "\n",
    "The rest of the code is the similar to the [First steps with `fluke` API](fluke_quick_api.ipynb) tutorial. We just replace `ClassificationEval` with our custom evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.data import DataSplitter\n",
    "from fluke.data.datasets import Datasets\n",
    "from fluke.evaluation import ClassificationEval\n",
    "from fluke import DDict\n",
    "from fluke.utils.log import Log\n",
    "from fluke import FlukeENV\n",
    "\n",
    "env = FlukeENV()\n",
    "env.set_seed(42) # we set a seed for reproducibility\n",
    "env.set_eval_cfg(DDict(pre_fit=True, post_fit=True)) \n",
    "env.set_evaluator(ClassificationEval(eval_every=1, n_classes=10))\n",
    "\n",
    "dataset = Datasets.get(\"mnist\", path=\"./data\")\n",
    "splitter = DataSplitter(dataset=dataset, distribution=\"dir\",\n",
    "                        client_split=0.1, dist_args=DDict(beta=0.5))\n",
    "\n",
    "client_hp = DDict(\n",
    "    batch_size=10,\n",
    "    local_epochs=5,\n",
    "    loss=\"CrossEntropyLoss\",\n",
    "    optimizer=DDict(\n",
    "      lr=0.01,\n",
    "      momentum=0.9,\n",
    "      weight_decay=0.0001),\n",
    "    scheduler=DDict(\n",
    "      gamma=1,\n",
    "      step_size=1)\n",
    ")\n",
    "\n",
    "# we put together the hyperparameters for the algorithm\n",
    "hyperparams = DDict(client=client_hp,\n",
    "                    server=DDict(weighted=True),\n",
    "                    model=\"MNIST_2NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the new federated algorithm comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = MyFLAlgorithm(n_clients=10, # 10 clients in the federation\n",
    "                          data_splitter=splitter,\n",
    "                          hyper_params=hyperparams)\n",
    "\n",
    "logger = Log()\n",
    "algorithm.set_callbacks(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only just need to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "algorithm.run(n_rounds=100, eligible_perc=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
