{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New federated Evaluation with `fluke`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through the steps required to implement a new evaluation that can be tested with ``fluke``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install `fluke` (if not already done)\n",
    "\n",
    "```bash\n",
    "pip install fluke-fl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted accuracy\n",
    "\n",
    "In this tutorial, we will show how to implement a metric that is quite common in Personalized Federated Learning in Fluke! In particular, a common technique for evaluating the local model is through using a balanced test set, weighting the accuracy based on the number of samples of each class. Intuitively, the weighted accuracy takes into account the number of samples for each class, allowing to lower the penalty if an error occurs in classifying a less frequent class. This metric is taken from \"Tackling Data Heterogeneity in Federated Learning with Class Prototypes\", Dai et al. and it is defined as follows:\n",
    "$$\n",
    "acc_i = \\frac{\\sum_{x_j,y_j\\in \\mathcal{D}_{test}}\\alpha_i(y_j)\\mathbb{1}(y_j = \\hat{y}_j)}{\\sum_{x_j,y_j\\in \\mathcal{D}_{test}}\\alpha_i(y_j)}\n",
    "$$\n",
    "\n",
    "where $\\alpha_i(\\cdot)$ is a positive valued function. It is defined as the probability that the sample y is from class c in the $i^{th}$ client. Notice that, for $\\alpha_i(\\cdot) = 1$ we obtain the traditional accuracy. In this tutorial, we will interpret $\\alpha_i(\\cdot)$ as the proportion of the local samples of the class $y$ over all the sample of that client. Specifically, we calculate the aforementioned coefficient for client $i$ and class $y_j$ as $\\alpha_i(y_j) = \\frac{Y^i_j}{Y^i}$, where $Y^i_j$ is the number of samples of class $y_j$ for client $i$ (training set), and $Y^i$ is the total number of examples of client $i$.\n",
    "\n",
    "In our case $\\mathcal{D}_{test}$ will be the dataset on the server, that is (usually) the original dataset test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the server-side logic\n",
    "\n",
    "Notice that `server.evaluation` is called in `server.fit` with only two arguments (the evaluator the eligible clients). As a consequence, if we want to modify the `server.evaluate` to take into account the class weights, we should modify the `server.fit` as well. However, this is too verbose. The most straightforward solution is to not modify `server.fit` and the input arguments of `server.evaluate` function, but modify the evaluator `evaluator.evaluate` input arguments, adding the class weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from fluke.client import Client\n",
    "from fluke.server import Server\n",
    "import numpy as np\n",
    "import torch\n",
    "from fluke.data import FastDataLoader  # NOQA\n",
    "from fluke.evaluation import Evaluator  # NOQA\n",
    "\n",
    "class MyServer(Server):\n",
    "\n",
    "    def evaluate(self, evaluator: Evaluator, test_set: FastDataLoader) -> dict[str, float]:\n",
    "        if self.test_set is not None:\n",
    "            return evaluator.evaluate(self.rounds + 1, self.model, self.test_set, device=self.device, weights=torch.ones(evaluator.n_classes))\n",
    "        return {}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the client-side logic\n",
    "\n",
    "Following the same logic as the server, we modify the `evaluator.evaluate` instead of the whole `client.local_update` and the inputs of `client.evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.utils import OptimizerConfigurator, clear_cache  # NOQA\n",
    "from torch.nn import Module\n",
    "from typing import Any\n",
    "\n",
    "class MyClient(Client):\n",
    "\n",
    "    def __init__(self,\n",
    "                 index: int,\n",
    "                 train_set: FastDataLoader,\n",
    "                 test_set: FastDataLoader,\n",
    "                 optimizer_cfg: OptimizerConfigurator,\n",
    "                 loss_fn: Module,\n",
    "                 local_epochs: int = 3,\n",
    "                 **kwargs: dict[str, Any]):\n",
    "        super().__init__(index,\n",
    "                 train_set,\n",
    "                 test_set,\n",
    "                 optimizer_cfg,\n",
    "                 loss_fn,\n",
    "                 local_epochs,\n",
    "                 **kwargs)\n",
    "        self.class_weights = torch.bincount(self.train_set.tensors[1])/self.train_set.size\n",
    "\n",
    "        \n",
    "    def evaluate(self, evaluator: Evaluator, test_set: FastDataLoader) -> dict[str, float]: \n",
    "        if self.model is not None:\n",
    "            return evaluator.evaluate(self._last_round, self.model, self.server.test_set, device=self.device, weights=self.class_weights)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the metric\n",
    "\n",
    "In the following, we start from the `classification` metric present in `eval.py` and modify it, taking into account the weight for each class. As a sanity check, in the global evaluation `accuracy` and `weighted accuracy` will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Iterable, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class Weight_classification_Eval(Evaluator):\n",
    "\n",
    "    def __init__(self, eval_every: int, n_classes: int):\n",
    "        super().__init__(eval_every=eval_every)\n",
    "        self.n_classes: int = n_classes\n",
    "\n",
    "    def evaluate(self,\n",
    "                 round: int,\n",
    "                 model: torch.nn.Module,\n",
    "                 eval_data_loader: Union[FastDataLoader,\n",
    "                                         Iterable[FastDataLoader]],\n",
    "                 loss_fn: Optional[torch.nn.Module] = None,\n",
    "                 device: torch.device = torch.device(\"cpu\"),\n",
    "                 weights: torch.tensor = None) -> dict:\n",
    "        from fluke.utils import clear_cache  # NOQA\n",
    "\n",
    "        if round % self.eval_every != 0:\n",
    "            return {}\n",
    "\n",
    "        if (model is None) or (eval_data_loader is None):\n",
    "            return {}\n",
    "\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        task = \"multiclass\"  # if self.n_classes >= 2 else \"binary\"\n",
    "        accs, losses = [], []\n",
    "        true_weights, pred_weights, mask = [], [], []\n",
    "        weight_accs = []\n",
    "        loss, cnt = 0, 0\n",
    "\n",
    "        if not isinstance(eval_data_loader, list):\n",
    "            eval_data_loader = [eval_data_loader]\n",
    "\n",
    "        for data_loader in eval_data_loader:\n",
    "            accuracy = Accuracy(task=task, num_classes=self.n_classes, top_k=1, average=\"micro\")\n",
    "            loss = 0\n",
    "            for X, y in data_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                with torch.no_grad():\n",
    "                    y_hat = model(X)\n",
    "                    if loss_fn is not None:\n",
    "                        loss += loss_fn(y_hat, y).item()\n",
    "                    y_hat = torch.max(y_hat, dim=1)[1]\n",
    "                true_weights.append(weights[y])\n",
    "                pred_weights.append(weights[y_hat])\n",
    "                mask.append(torch.eq(y, y_hat))\n",
    "                accuracy.update(y_hat.cpu(), y.cpu())\n",
    "               \n",
    "            \n",
    "            true_weights = torch.cat(true_weights, dim=0)\n",
    "            pred_weights = torch.cat(pred_weights, dim=0)\n",
    "            mask = torch.cat(mask, dim=0)\n",
    "            pred_weights = pred_weights*mask\n",
    "            weight_accs.append(pred_weights.sum().item()/true_weights.sum().item())\n",
    "            \n",
    "            cnt += len(data_loader)\n",
    "            accs.append(accuracy.compute().item())\n",
    "            losses.append(loss / cnt)\n",
    "\n",
    "        model.to(\"cpu\")\n",
    "        clear_cache()\n",
    "\n",
    "        result = {\n",
    "            \"accuracy\":  np.round(sum(accs) / len(accs), 5).item(),\n",
    "            \"weighted_accuracy\":  np.round(sum(weight_accs) / len(weight_accs), 5).item(),\n",
    "        }\n",
    "        if loss_fn is not None:\n",
    "            result[\"loss\"] = np.round(sum(losses) / len(losses), 5).item()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(eval_every={self.eval_every}\" + \\\n",
    "               f\", n_classes={self.n_classes})[accuracy, weight_acc]\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the new metric\n",
    "\n",
    "Now, we are ready to test our metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.algorithms import CentralizedFL\n",
    "\n",
    "class MyFLAlgorithm(CentralizedFL):\n",
    "\n",
    "    def get_client_class(self) -> Client:\n",
    "        return MyClient\n",
    "\n",
    "    def get_server_class(self) -> Server:\n",
    "        return MyServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to test the new federated algorithm\n",
    "\n",
    "The rest of the code is the similar to the [First steps with `fluke` API](fluke_quick_api.ipynb) tutorial. We just replace `classification_eval` with our custom evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelefonio/Desktop/fluke-development/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fluke.data import DataSplitter\n",
    "from fluke.data.datasets import Datasets\n",
    "from fluke import DDict\n",
    "from fluke.utils.log import Log\n",
    "from fluke import GlobalSettings\n",
    "\n",
    "settings = GlobalSettings()\n",
    "settings.set_seed(42) # we set a seed for reproducibility\n",
    "settings.set_device(\"cpu\") # we use the CPU for this example\n",
    "\n",
    "settings.get_eval_cfg().pre_fit = True\n",
    "settings.get_eval_cfg().post_fit = True\n",
    "\n",
    "dataset = Datasets.get(\"mnist\", path=\"./data\")\n",
    "\n",
    "# we set the evaluator to be used by both the server and the clients\n",
    "settings.set_evaluator(Weight_classification_Eval(eval_every=1, n_classes=dataset.num_classes))\n",
    "\n",
    "splitter = DataSplitter(dataset=dataset,\n",
    "                        distribution=\"iid\")\n",
    "\n",
    "client_hp = DDict(\n",
    "    batch_size=10,\n",
    "    local_epochs=5,\n",
    "    loss=\"CrossEntropyLoss\",\n",
    "    optimizer=DDict(\n",
    "      lr=0.01,\n",
    "      momentum=0.9,\n",
    "      weight_decay=0.0001),\n",
    "    scheduler=DDict(\n",
    "      gamma=1,\n",
    "      step_size=1)\n",
    ")\n",
    "\n",
    "# we put together the hyperparameters for the algorithm\n",
    "hyperparams = DDict(client=client_hp,\n",
    "                    server=DDict(weighted=True),\n",
    "                    model=\"MNIST_2NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the new federated algorithm comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = MyFLAlgorithm(n_clients=10, # 10 clients in the federation\n",
    "                          data_splitter=splitter,\n",
    "                          hyper_params=hyperparams)\n",
    "\n",
    "settings.set_evaluator(Weight_classification_Eval(eval_every=1, n_classes=dataset.num_classes))\n",
    "\n",
    "logger = Log()\n",
    "algorithm.set_callbacks(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only just need to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/samuelefonio/Desktop/fluke-development/venv/lib/python3.10/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/samuelefonio/Desktop/fluke-development/venv/lib/python3.10/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────── Round: 1 ────────────────────────────────────────────────────╮\n",
       "│ <span style=\"font-weight: bold\">{</span>                                                                                                               │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'pre-fit'</span>: <span style=\"font-weight: bold\">{</span>                                                                                                │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0895</span>,                                                                                     │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'weighted_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08351</span>                                                                            │\n",
       "│     <span style=\"font-weight: bold\">}</span>,                                                                                                          │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'post-fit'</span>: <span style=\"font-weight: bold\">{</span>                                                                                               │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9447</span>,                                                                                     │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'weighted_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.94573</span>                                                                            │\n",
       "│     <span style=\"font-weight: bold\">}</span>,                                                                                                          │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'global'</span>: <span style=\"font-weight: bold\">{</span>                                                                                                 │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9526</span>,                                                                                     │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'weighted_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9526</span>                                                                             │\n",
       "│     <span style=\"font-weight: bold\">}</span>,                                                                                                          │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'comm_cost'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1781100</span>                                                                                        │\n",
       "│ <span style=\"font-weight: bold\">}</span>                                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────── Round: 1 ────────────────────────────────────────────────────╮\n",
       "│ \u001b[1m{\u001b[0m                                                                                                               │\n",
       "│     \u001b[32m'pre-fit'\u001b[0m: \u001b[1m{\u001b[0m                                                                                                │\n",
       "│         \u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.0895\u001b[0m,                                                                                     │\n",
       "│         \u001b[32m'weighted_accuracy'\u001b[0m: \u001b[1;36m0.08351\u001b[0m                                                                            │\n",
       "│     \u001b[1m}\u001b[0m,                                                                                                          │\n",
       "│     \u001b[32m'post-fit'\u001b[0m: \u001b[1m{\u001b[0m                                                                                               │\n",
       "│         \u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.9447\u001b[0m,                                                                                     │\n",
       "│         \u001b[32m'weighted_accuracy'\u001b[0m: \u001b[1;36m0.94573\u001b[0m                                                                            │\n",
       "│     \u001b[1m}\u001b[0m,                                                                                                          │\n",
       "│     \u001b[32m'global'\u001b[0m: \u001b[1m{\u001b[0m                                                                                                 │\n",
       "│         \u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.9526\u001b[0m,                                                                                     │\n",
       "│         \u001b[32m'weighted_accuracy'\u001b[0m: \u001b[1;36m0.9526\u001b[0m                                                                             │\n",
       "│     \u001b[1m}\u001b[0m,                                                                                                          │\n",
       "│     \u001b[32m'comm_cost'\u001b[0m: \u001b[1;36m1781100\u001b[0m                                                                                        │\n",
       "│ \u001b[1m}\u001b[0m                                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Memory usage: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.51</span> %\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Memory usage: \u001b[1;36m4.51\u001b[0m %\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────── Round: 2 ────────────────────────────────────────────────────╮\n",
       "│ <span style=\"font-weight: bold\">{</span>                                                                                                               │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'pre-fit'</span>: <span style=\"font-weight: bold\">{</span>                                                                                                │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9526</span>,                                                                                     │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'weighted_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.95352</span>                                                                            │\n",
       "│     <span style=\"font-weight: bold\">}</span>,                                                                                                          │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'post-fit'</span>: <span style=\"font-weight: bold\">{</span>                                                                                               │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9529</span>,                                                                                     │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'weighted_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.95462</span>                                                                            │\n",
       "│     <span style=\"font-weight: bold\">}</span>,                                                                                                          │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'global'</span>: <span style=\"font-weight: bold\">{</span>                                                                                                 │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9702</span>,                                                                                     │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'weighted_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9702</span>                                                                             │\n",
       "│     <span style=\"font-weight: bold\">}</span>,                                                                                                          │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'comm_cost'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1781100</span>                                                                                        │\n",
       "│ <span style=\"font-weight: bold\">}</span>                                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────── Round: 2 ────────────────────────────────────────────────────╮\n",
       "│ \u001b[1m{\u001b[0m                                                                                                               │\n",
       "│     \u001b[32m'pre-fit'\u001b[0m: \u001b[1m{\u001b[0m                                                                                                │\n",
       "│         \u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.9526\u001b[0m,                                                                                     │\n",
       "│         \u001b[32m'weighted_accuracy'\u001b[0m: \u001b[1;36m0.95352\u001b[0m                                                                            │\n",
       "│     \u001b[1m}\u001b[0m,                                                                                                          │\n",
       "│     \u001b[32m'post-fit'\u001b[0m: \u001b[1m{\u001b[0m                                                                                               │\n",
       "│         \u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.9529\u001b[0m,                                                                                     │\n",
       "│         \u001b[32m'weighted_accuracy'\u001b[0m: \u001b[1;36m0.95462\u001b[0m                                                                            │\n",
       "│     \u001b[1m}\u001b[0m,                                                                                                          │\n",
       "│     \u001b[32m'global'\u001b[0m: \u001b[1m{\u001b[0m                                                                                                 │\n",
       "│         \u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.9702\u001b[0m,                                                                                     │\n",
       "│         \u001b[32m'weighted_accuracy'\u001b[0m: \u001b[1;36m0.9702\u001b[0m                                                                             │\n",
       "│     \u001b[1m}\u001b[0m,                                                                                                          │\n",
       "│     \u001b[32m'comm_cost'\u001b[0m: \u001b[1;36m1781100\u001b[0m                                                                                        │\n",
       "│ \u001b[1m}\u001b[0m                                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Memory usage: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.77</span> %\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Memory usage: \u001b[1;36m4.77\u001b[0m %\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────────────────────────────────────────── Overall Performance ──────────────────────────────────────────────╮\n",
       "│ <span style=\"font-weight: bold\">{</span>                                                                                                               │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'pre-fit'</span>: <span style=\"font-weight: bold\">{</span>                                                                                                │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9702</span>,                                                                                     │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'weighted_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.97051</span>                                                                            │\n",
       "│     <span style=\"font-weight: bold\">}</span>,                                                                                                          │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'post-fit'</span>: <span style=\"font-weight: bold\">{</span>                                                                                               │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9529</span>,                                                                                     │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'weighted_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.95462</span>                                                                            │\n",
       "│     <span style=\"font-weight: bold\">}</span>,                                                                                                          │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'global'</span>: <span style=\"font-weight: bold\">{</span>                                                                                                 │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9702</span>,                                                                                     │\n",
       "│         <span style=\"color: #008000; text-decoration-color: #008000\">'weighted_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9702</span>                                                                             │\n",
       "│     <span style=\"font-weight: bold\">}</span>                                                                                                           │\n",
       "│ <span style=\"font-weight: bold\">}</span>                                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────────────────────────────────────────────── Overall Performance ──────────────────────────────────────────────╮\n",
       "│ \u001b[1m{\u001b[0m                                                                                                               │\n",
       "│     \u001b[32m'pre-fit'\u001b[0m: \u001b[1m{\u001b[0m                                                                                                │\n",
       "│         \u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.9702\u001b[0m,                                                                                     │\n",
       "│         \u001b[32m'weighted_accuracy'\u001b[0m: \u001b[1;36m0.97051\u001b[0m                                                                            │\n",
       "│     \u001b[1m}\u001b[0m,                                                                                                          │\n",
       "│     \u001b[32m'post-fit'\u001b[0m: \u001b[1m{\u001b[0m                                                                                               │\n",
       "│         \u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.9529\u001b[0m,                                                                                     │\n",
       "│         \u001b[32m'weighted_accuracy'\u001b[0m: \u001b[1;36m0.95462\u001b[0m                                                                            │\n",
       "│     \u001b[1m}\u001b[0m,                                                                                                          │\n",
       "│     \u001b[32m'global'\u001b[0m: \u001b[1m{\u001b[0m                                                                                                 │\n",
       "│         \u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.9702\u001b[0m,                                                                                     │\n",
       "│         \u001b[32m'weighted_accuracy'\u001b[0m: \u001b[1;36m0.9702\u001b[0m                                                                             │\n",
       "│     \u001b[1m}\u001b[0m                                                                                                           │\n",
       "│ \u001b[1m}\u001b[0m                                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────── Total communication cost ────────────────────────────────────────────╮\n",
       "│ <span style=\"font-weight: bold\">{</span>                                                                                                               │\n",
       "│     <span style=\"color: #008000; text-decoration-color: #008000\">'comm_costs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4808970</span>                                                                                       │\n",
       "│ <span style=\"font-weight: bold\">}</span>                                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────── Total communication cost ────────────────────────────────────────────╮\n",
       "│ \u001b[1m{\u001b[0m                                                                                                               │\n",
       "│     \u001b[32m'comm_costs'\u001b[0m: \u001b[1;36m4808970\u001b[0m                                                                                       │\n",
       "│ \u001b[1m}\u001b[0m                                                                                                               │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "algorithm.run(n_rounds=100, eligible_perc=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluke311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
