{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a custom model in `fluke`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through the steps required to use a custom federated neural network in `fluke`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install `fluke` (if not already done)\n",
    "\n",
    "```bash\n",
    "pip install fluke-fl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your neural network\n",
    "\n",
    "For the purpose of this tutorial, we will define a very simple neural network for the MNIST dataset. \n",
    "The network will have two hidden layers with ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "\n",
    "class MyMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 100)\n",
    "        self.fc2 = torch.nn.Linear(100, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAvg with your custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are almost ready to use your custom model in `fluke`. \n",
    "The only thing you need to do is to set an instance of your `MyMLP` as model in the hyper-parameters of the algorithm. \n",
    "\n",
    "There is also another possibility, that is to provide as model the fully qualified name of your model class. \n",
    "This is useful because it allows to use a custom model with the `fluke` command line interface. \n",
    "\n",
    "To keep it simple, we are going to use FedAVG, but you can use any other algorithm available in `fluke` or even implement your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.data import DataSplitter\n",
    "from fluke.data.datasets import Datasets\n",
    "from fluke import DDict\n",
    "from fluke.utils.log import Log\n",
    "from fluke.algorithms.fedavg import FedAVG\n",
    "from fluke.evaluation import ClassificationEval\n",
    "from fluke import GlobalSettings\n",
    "\n",
    "settings = GlobalSettings()\n",
    "settings.set_seed(42) # we set a seed for reproducibility\n",
    "settings.set_device(\"cpu\") # we use the CPU for this example\n",
    "\n",
    "dataset = Datasets.get(\"mnist\", path=\"./data\")\n",
    "\n",
    "# we set the evaluator to be used by both the server and the clients\n",
    "settings.set_evaluator(ClassificationEval(eval_every=1, n_classes=dataset.num_classes))\n",
    "\n",
    "splitter = DataSplitter(dataset=dataset,\n",
    "                        distribution=\"iid\")\n",
    "\n",
    "client_hp = DDict(\n",
    "    batch_size=10,\n",
    "    local_epochs=5,\n",
    "    loss=\"CrossEntropyLoss\",\n",
    "    optimizer=DDict(\n",
    "        lr=0.01,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0001),\n",
    "    scheduler=DDict(\n",
    "        gamma=1,\n",
    "        step_size=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where you must set the model in the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = DDict(client=client_hp,\n",
    "                    server=DDict(weighted=True),\n",
    "                    model=MyMLP()) # or model=\"__main__.MyMLP\"\n",
    "                                   # or model=\"mymodule.MyMLP\" if the model is in a module called mymodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's initialize the algorithm and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = FedAVG(n_clients=10, # 10 clients in the federation\n",
    "                   data_splitter=splitter,\n",
    "                   hyper_params=hyperparams)\n",
    "\n",
    "logger = Log()\n",
    "algorithm.set_callbacks(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "algorithm.run(n_rounds=10, eligible_perc=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
